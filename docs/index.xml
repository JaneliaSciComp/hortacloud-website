<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>HortaCloud – Documentation</title><link>https://hortacloud.janelia.org/docs/</link><description>Recent content in Documentation on HortaCloud</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://hortacloud.janelia.org/docs/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Deployment</title><link>https://hortacloud.janelia.org/docs/administration/aws/deployment/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hortacloud.janelia.org/docs/administration/aws/deployment/</guid><description>
&lt;p>The deployment uses AWS CDK to create AWS resources on your AWS account as shown in the diagram below. All services run in a secured Virtual Private Cloud (VPC).&lt;/p>
&lt;a href="../cloud_deployment.png">
&lt;img
src="../cloud_deployment.png"
class="rounded mx-auto"
style="max-width: 700px;"
alt="cloud deployment diagram"/>
&lt;/a>
&lt;h2 id="install-prerequisites">Install prerequisites&lt;/h2>
&lt;p>You should have &lt;strong>node v14&lt;/strong> installed on your local machine. We recommend using &lt;a href="https://github.com/nvm-sh/nvm">nvm&lt;/a> to install and activate this version of node.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Install AWS CLI&lt;/p>
&lt;ul>
&lt;li>AWS CDK requires AWS CLI to be installed and configured on the computer from which one runs the deployment procedure. &lt;a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html">Installation&lt;/a> &amp;amp; &lt;a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html">configuration&lt;/a> instructions can be found in the AWS documentation.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Install AWS CDK by running&lt;/p>
&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>npm install
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This command will install CDK in a development environment so you can access its help as below (notice the &amp;lsquo;&amp;ndash;&amp;rsquo; separator between cdk and cdk options - this is specific to npm not to cdk so all CDK flags must be after the double hyphen separator):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>npm run cdk -- --help
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="get-the-deployment-scripts">Get the deployment scripts&lt;/h2>
&lt;p>Clone the repository containing the deployment scripts:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>git clone https://github.com/JaneliaSciComp/hortacloud/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> hortacloud
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Install the dependencies:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>npm run setup
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="configure-environment">Configure environment&lt;/h2>
&lt;p>The following values must be set in the &lt;code>.env&lt;/code> file:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-env" data-lang="env">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">AWS_REGION&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&amp;lt;your aws region&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">AWS_ACCOUNT&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&amp;lt;your aws account&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">HORTA_ORG&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&amp;lt;app qualifier name&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">ADMIN_USER_EMAIL&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&amp;lt;admin email&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">JACS_JWT_KEY&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&amp;lt;a &lt;span style="color:#0000cf;font-weight:bold">32&lt;/span> byte jwt secret&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">JACS_MONGO_KEY&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&amp;lt;a &lt;span style="color:#0000cf;font-weight:bold">32&lt;/span> byte mongo secret&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">JACS_APP_PASSWD&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&amp;lt;app password&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">RABBITMQ_PASSWD&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&amp;lt;rabbitmq password&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">JACS_API_KEY&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&amp;lt;jacs api key&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">JADE_API_KEY&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&amp;lt;jade api key&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">HORTA_DATA_BUCKETS&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&amp;lt;s3 buckets that hold MouseLight data&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The api keys and secrets have been randomly generated during the setup step, but you can generate new ones with the following command:&lt;/p>
&lt;pre tabindex="0">&lt;code>openssl rand -hex 32
&lt;/code>&lt;/pre>&lt;p>We prefer this procedure because these values will be handled during the installation using the &lt;code>sed&lt;/code> command and it is preferable that they not contain any characters that require escaping in a sed command.&lt;/p>
&lt;p>If you already have data on some S3 buckets you can add them to &lt;code>HORTA_DATA_BUCKETS&lt;/code> as a comma separated list. For example, if you want to use Janelia&amp;rsquo;s Open Data bucket but in addition you also have your data on a private bucket (&amp;lsquo;janelia-mouseligh-demo&amp;rsquo; in this example) you need to set &lt;code>HORTA_DATA_BUCKETS=&amp;quot;janelia-mouselight-imagery,janelia-mouselight-demo&amp;quot;&lt;/code>. Currently it is set to Janelia&amp;rsquo;s open data MouseLight bucket only. Every bucket specified in the &amp;lsquo;HORTA_DATA_BUCKETS&amp;rsquo; list will be available in the workstation as &lt;code>/&amp;lt;s3BucketName&amp;gt;&lt;/code> directory.&lt;/p>
&lt;p>If you want to change the setting for &lt;code>HORTA_WS_INSTANCE_TYPE&lt;/code>, keep in mind that you may have to change &lt;code>HORTA_WS_IMAGE_NAME&lt;/code>
For &lt;code>HORTA_WS_INSTANCE_TYPE&lt;/code> set to any &lt;code>stream.graphics.g4dn.*&lt;/code> instances:&lt;/p>
&lt;ul>
&lt;li>&lt;code>stream.graphics.g4dn.xlarge&lt;/code>&lt;/li>
&lt;li>&lt;code>stream.graphics.g4dn.2xlarge&lt;/code>&lt;/li>
&lt;li>&lt;code>stream.graphics.g4dn.4xlarge&lt;/code>&lt;/li>
&lt;li>&lt;code>stream.graphics.g4dn.8xlarge&lt;/code>&lt;/li>
&lt;li>&lt;code>stream.graphics.g4dn.12xlarge&lt;/code>&lt;/li>
&lt;li>&lt;code>stream.graphics.g4dn.16xlarge&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>use: &lt;code>HORTA_WS_IMAGE_NAME=AppStream-Graphics-G4dn-WinServer2019-07-19-2021&lt;/code> image.&lt;/p>
&lt;p>For &lt;code>HORTA_WS_INSTANCE_TYPE&lt;/code> set to any &lt;code>stream.graphics-pro.*&lt;/code> instances:&lt;/p>
&lt;ul>
&lt;li>&lt;code>stream.graphics-pro.4xlarge&lt;/code>&lt;/li>
&lt;li>&lt;code>stream.graphics-pro.8xlarge&lt;/code>&lt;/li>
&lt;li>&lt;code>stream.graphics-pro.16xlarge&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>use &lt;code>HORTA_WS_IMAGE_NAME=AppStream-Graphics-Pro-WinServer2019-10-08-2021&lt;/code> image&lt;/p>
&lt;h2 id="configure-aws-account">Configure AWS account&lt;/h2>
&lt;h3 id="iam-required-roles">IAM Required Roles&lt;/h3>
&lt;p>In order to create an AppStream Image Builder, which is needed to create the Workstation Image, you need to have all &lt;a href="https://docs.aws.amazon.com/appstream2/latest/developerguide/roles-required-for-appstream.html">roles required by AppStream&lt;/a>. Check that by simply connecting to the AWS console and check if the Roles are available in the IAM Service - select &amp;ldquo;Services&amp;rdquo; &amp;gt; &amp;ldquo;Security, Identity, Compliance&amp;rdquo; &amp;gt; &amp;ldquo;IAM&amp;rdquo; then verify that the required roles are present:&lt;/p>
&lt;ul>
&lt;li>AmazonAppStreamServiceAccess&lt;/li>
&lt;li>ApplicationAutoScalingForAmazonAppStreamAccess&lt;/li>
&lt;li>AWSServiceRoleForApplicationAutoScaling_AppStreamFleet&lt;/li>
&lt;/ul>
&lt;h3 id="aws-limits">AWS Limits&lt;/h3>
&lt;p>Most AWS services allow you to setup restrictions on the number of active instances. The default limits, especially for some AppStream resources, such as &amp;ldquo;Maximum ImageBuilders&amp;rdquo; for some graphics instances - &amp;ldquo;stream.graphics.g4dn.xlarge&amp;rdquo; may be really low (0 in some cases). Connect to AWS console &amp;ldquo;Service Quotas&amp;rdquo; service and increase the limit for in case you see a &lt;code>limit was exceeded&lt;/code> error. Typically take a look at the limits setup for your account for EC2, VPC, AppStream, S3. Keep in mind that limits may be different from instance type to instance type for AppStream service, so you may have to adjust the limits based on the AppStream instance type selection.&lt;/p>
&lt;h2 id="deploy-hortacloud-services-to-aws">Deploy HortaCloud services to AWS&lt;/h2>
&lt;p>After the setup is complete, deploy the application by running:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>npm run deploy
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>There are a few steps during the deployment that require manual intervention. The deploy script will indicate when these steps should be taken with a ⚠️ warning message.&lt;/p>
&lt;p>The full deployment of the application is done in 3 steps run automatically one after the other, but the second step requires manual intervention:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Deploy the back-end stacks&lt;/strong> - this includes the AppStream builder. At the back end deployment the installation process will also create the admin user configured in &lt;code>ADMIN_USER_EMAIL&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Connect to AppStream builder and install the workstation application&lt;/strong> - This is a semiautomated step that involves copying and running two PowerShell scripts onto the AppStream builder instance.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Deploy the user login and administration stack.&lt;/strong>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="workstation-app-installation">Workstation app installation&lt;/h3>
&lt;p>For client installation start and connect to the AppStream builder instance then copy the following scripts from this repo to the AppStream instance:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="vpc_stack/src/asbuilder/installcmd.ps1">installcmd.ps1&lt;/a> - installs JDK and the workstation&lt;/li>
&lt;li>&lt;a href="vpc_stack/src/asbuilder/createappimage.ps1">createappimage.ps1&lt;/a> - creates the AppStream image&lt;/li>
&lt;/ul>
&lt;p>After you copied or created the scripts:&lt;/p>
&lt;ul>
&lt;li>Log in to the AWS console and go to &lt;a href="https://console.aws.amazon.com/appstream2">https://console.aws.amazon.com/appstream2&lt;/a>&lt;/li>
&lt;li>Find your new builder in the &amp;ldquo;Images &amp;gt; Image Builder&amp;rdquo; tab&lt;/li>
&lt;li>Click on the image name and open an &amp;ldquo;Administrator&amp;rdquo; window by clicking on the &amp;ldquo;Connect&amp;rdquo; button.&lt;/li>
&lt;li>Copy the installation scripts from your local machine to AppStream:
&lt;ul>
&lt;li>Click on the folder icon at the top left of the window&lt;/li>
&lt;li>Select the &lt;code>Temporary Files&lt;/code> folder&lt;/li>
&lt;li>Use the &lt;code>Upload Files&lt;/code> icon to find the files on your machine and upload them.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Open the powershell by typing &amp;ldquo;`Power shell&amp;rdquo; in the search found at the bottom left of the window. This step used to require an &amp;ldquo;Administrator Power Shell&amp;rdquo; but now it needs only a regular user power shell and it may actually fail the install if you run it in an Administrator Power Shell.&lt;/li>
&lt;li>Change to the directory where you uploaded the installation scripts, eg:&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code class="language-windows" data-lang="windows">cd &amp;#39;C:\Users\ImagebuilderAdmin\My Files\Temporary Files&amp;#39;
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>Run the installcmd script to install the workstation. &amp;lt;serverName&amp;gt; is the name of the backend EC2 instance, typically it looks like &lt;code>ip-&amp;lt;ip4 with dashes instead of dots&amp;gt;.ec2.internal&lt;/code>. Instructions for locating this are provided as output from the installer script.&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code class="language-windows" data-lang="windows">installcmd.ps1 &amp;lt;serverName&amp;gt;
&lt;/code>&lt;/pre>&lt;p>This will install the JDK and the workstation. The installer will run silently and it will install the workstation under the &lt;code>C:\apps&lt;/code> folder. If it prompts you for the install directory, select &lt;code>C:\apps&lt;/code> as the JaneliaWorkstation location.&lt;/p>
&lt;ul>
&lt;li>&lt;em>Optional&lt;/em> - To start the workstation for testing, run:&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code class="language-windows" data-lang="windows">c:\apps\runJaneliaWorkstation.ps1
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>
&lt;p>when prompted, login as the admin user you set in ADMIN_USER_EMAIL (leave the password empty)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Navigate through the menus to make sure the workstation is working. &lt;em>Do not create any user accounts at this time as they will get created from the Admin web application.&lt;/em>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>When testing is finished, close down the workstation.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Finalize the creation of the AppStream image, run:&lt;/p>
&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code class="language-windows" data-lang="windows">createappimage.ps1
&lt;/code>&lt;/pre>&lt;p>Keep in mind that once you start this step the builder instance begins the snap shotting process and it will not be usable until it completes. After this is completed the AppStream image should be available and the builder will be in a stop state. To use it again you need to start it and then you can connect.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>You can now safely close the AppStream session and return to the AppStream console. There you will see a new image in the image registry with a status of &lt;code>Pending&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Once the image status has changed to a status of &lt;code>Available&lt;/code> you can start the fleet by going to the &lt;code>Fleets&lt;/code> page on the AppStream site.&lt;/p>
&lt;ul>
&lt;li>Select your fleet from the list of fleets and then select &amp;lsquo;Start&amp;rsquo; from the &lt;code>Action&lt;/code> menu.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>At this point the installation script you started on your host machine, should continue to completion.&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>Docs: Getting started</title><link>https://hortacloud.janelia.org/docs/development/getting_started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hortacloud.janelia.org/docs/development/getting_started/</guid><description/></item><item><title>Docs: Single server deployment</title><link>https://hortacloud.janelia.org/docs/development/composedeployment/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hortacloud.janelia.org/docs/development/composedeployment/</guid><description>
&lt;div class="pb-3">
&lt;a href="single_server.png">
&lt;img
src="../single_server.png"
class="rounded mx-auto"
style="max-width: 700px;"
alt="single sever deployment diagram"/>
&lt;/a>
&lt;/div>
&lt;p>This document describes a Janelia Workstation deployment intended for setting up a personal development environment. Unlike the canonical distributed deployments which use Docker Swarm, this deployment uses Docker Compose to orchestrate the services on a single server.&lt;/p>
&lt;p>Note that this deployment does not build and serve the Workstation client installers, although that could certainly be added in cases where those pieces need to be developed and tested. In most cases, however, it is expected that this server-side deployment be paired with a development client built and run directly from IntelliJ or NetBeans.&lt;/p>
&lt;h2 id="system-setup">System Setup&lt;/h2>
&lt;p>This deployment should work on any system where Docker is supported. Currently, it has only been tested on Scientific Linux 7 and macOS Mojave.&lt;/p>
&lt;p>To install Docker and Docker Compose on Oracle Linux 8, follow &lt;a href="../installingdocker">these instructions&lt;/a>.&lt;/p>
&lt;h2 id="clone-this-repo">Clone This Repo&lt;/h2>
&lt;p>Begin by cloning this repo:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>git clone https://github.com/JaneliaSciComp/jacs-cm.git
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> jacs-cm
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="configure-the-system">Configure The System&lt;/h2>
&lt;p>Next, create a &lt;code>.env.config&lt;/code> file inside the jacs-cm directory. This file defines the environment (usernames, passwords, etc.) You can copy the template to get started:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cp .env.template .env.config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>vi .env.config
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>At minimum, you must customize the following:&lt;/p>
&lt;ol>
&lt;li>Configured the &lt;code>UNAME&lt;/code> and &lt;code>GNAME&lt;/code> to your liking. Ideally, these should be your username and primary group.&lt;/li>
&lt;li>Setup &lt;code>REDUNDANT_STORAGE&lt;/code> and &lt;code>NON_REDUNDANT_STORAGE&lt;/code> to point to directories accessible by &lt;code>UNAME&lt;/code>:&lt;code>GNAME&lt;/code>. If you want to use the defaults, you may need to create these directories and set the permissions yourself.&lt;/li>
&lt;li>Set &lt;code>HOST1&lt;/code> to the hostname you are deploying on. If possible, use a fully-qualified hostname &amp;ndash; it should match the SSL certificate you intend to use.&lt;/li>
&lt;li>Fill in all the unset passwords with &amp;gt;8 character passwords. You should only use alphanumeric characters, special characters are not currently supported.&lt;/li>
&lt;li>Generate 32-byte secret keys for JWT_SECRET_KEY and MONGODB_SECRET_KEY.&lt;/li>
&lt;/ol>
&lt;h2 id="enable-databases-optional">Enable Databases (optional)&lt;/h2>
&lt;p>Currently, Janelia runs MongoDB outside of the Swarm, so they are commented out in the deployment. If you&amp;rsquo;d like to run the databases as part of the swarm, edit the yaml files under ./deployments/jacs/ and uncomment the databases.&lt;/p>
&lt;h2 id="initialize-filesystems">Initialize Filesystems&lt;/h2>
&lt;p>The first step is to initialize the filesystem. Ensure that your &lt;code>REDUNDANT_STORAGE&lt;/code> (default: /opt/jacs), &lt;code>NON_REDUNDANT_STORAGE&lt;/code> (default: /data) directories exist and can be written to by your UNAME:GNAME user (default: docker-nobody).
If you are using Docker for Mac, you&amp;rsquo;ll need to take the additional step of configuring share paths at Docker -&amp;gt; Preferences&amp;hellip; -&amp;gt; File Sharing. Add both &lt;code>REDUNDANT_STORAGE&lt;/code> and &lt;code>NON_REDUNDANT_STORAGE&lt;/code> and then click &amp;ldquo;Apply &amp;amp; Restart&amp;rdquo; to save your changes.&lt;/p>
&lt;p>Next, run the filesystem initialization procedure:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>./manage.sh init-local-filesystem
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You should see output about directories being created and initialized. If there are any errors, they need to be resolved before moving further.&lt;/p>
&lt;p>Once the initialization is complete, you can manually edit the files found in &lt;code>CONFIG_DIR&lt;/code>. You can use these configuration files to customize much of the JACS environment.&lt;/p>
&lt;h3 id="ssl-certificates">SSL Certificates&lt;/h3>
&lt;p>At this point, &lt;strong>it is strongly recommended is to replace the self-signed certificates&lt;/strong> in &lt;code>CONFIG_DIR/certs/*&lt;/code> with your own certificates signed by a Certificate Authority:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>sudo cp /path/to/your/certs/cert.&lt;span style="color:#ce5c00;font-weight:bold">{&lt;/span>crt,key&lt;span style="color:#ce5c00;font-weight:bold">}&lt;/span> &lt;span style="color:#000">$CONFIG_DIR&lt;/span>/certs/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sudo chown docker-nobody:docker-nobody &lt;span style="color:#000">$CONFIG_DIR&lt;/span>/certs/*
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you use self-signed certificates, you will need to &lt;a href="../selfsignedcerts">set up the trust chain&lt;/a> for them later.&lt;/p>
&lt;h3 id="external-authentication">External Authentication&lt;/h3>
&lt;p>The JACS system has its own self-contained authentication system, and can manage users and passwords internally.&lt;/p>
&lt;p>If you&amp;rsquo;d prefer that users authenticate against your existing LDAP or ActiveDirectory server, edit &lt;code>$CONFIG_DIR/jacs-sync/jacs.properties&lt;/code> and add these properties:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>LDAP.URL&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>LDAP.SearchBase&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>LDAP.SearchFilter&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>LDAP.BindDN&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>LDAP.BindCredentials&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The URL should point to your authentication server. The SearchBase is part of a distinguished name to search, something like &amp;ldquo;ou=People,dc=yourorg,dc=org&amp;rdquo;. The SearchFilter is the attribute to search on, something like &amp;ldquo;(cn={{username}})&amp;rdquo;. BindDN and BindCredentials defines the distinguished name and password for a service user that can access user information like full names and emails.&lt;/p>
&lt;h2 id="start-all-containers">Start All Containers&lt;/h2>
&lt;p>Now you can bring up all of the application containers:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>./manage.sh compose up -d
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can monitor the progress with this command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>./manage.sh compose ps
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="initialize-databases">Initialize Databases&lt;/h2>
&lt;p>If you are running your own databases, you will need to initalize them now:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>./manage.sh init-databases
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="verify-functionality">Verify Functionality&lt;/h2>
&lt;p>You can verify the Authentication Service is working as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>./manage.sh login
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You should be able to log in with the default admin account (root/root), or any LDAP/AD account if you&amp;rsquo;ve configured external authentication. This will return a JWT that can be used on subsequent requests.·&lt;/p>
&lt;p>If you run into any problems, these &lt;a href="../troubleshooting">troubleshooting tips&lt;/a> may help.&lt;/p>
&lt;h2 id="updating-containers">Updating Containers&lt;/h2>
&lt;p>Containers in this deployment are automatically updated by &lt;a href="https://github.com/containrrr/watchtower">Watchtower&lt;/a> whenever a new one is available with the &amp;ldquo;latest&amp;rdquo; tag. To update the deployment, simply build and push a new container to the configured registry.&lt;/p>
&lt;h2 id="stop-all-containers">Stop All Containers&lt;/h2>
&lt;p>To stop all containers, run this command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>./manage.sh compose down
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="build-and-run-client">Build and Run Client&lt;/h2>
&lt;p>Now you can checkout the &lt;a href="https://github.com/JaneliaSciComp/workstation">Janelia Workstation&lt;/a> code base in IntelliJ or NetBeans and run its as per its README.&lt;/p>
&lt;p>The client will ask you for the API Gateway URL, which is just &lt;code>http://$HOST1&lt;/code>. In order to automatically connect to your standalone gateway instance, you can create a new file at &lt;code>workstation/modules/Core/src/main/resources/my.properties&lt;/code> with this content (replacing the variables with the values from your .env.config file):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>api.gateway&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>https://&lt;span style="color:#000">$HOST1&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Two-server deployment</title><link>https://hortacloud.janelia.org/docs/administration/bare_metal/twoserverdeployment/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hortacloud.janelia.org/docs/administration/bare_metal/twoserverdeployment/</guid><description>
&lt;p>This document describes the canonical two-server Janelia Workstation deployment for supporting neuron tracing for the &lt;a href="https://www.janelia.org/project-team/mouselight">MouseLight project&lt;/a> at the Janelia Research Campus and other research institutions. This deployment uses Docker Swarm to orchestrate prebuilt containers available on Docker Hub.&lt;/p>
&lt;h2 id="deployment-diagram">Deployment Diagram&lt;/h2>
&lt;a href="../TwoServerDeployment.png">
&lt;img
src="../TwoServerDeployment.png"
class="rounded mx-auto"
style="max-width: 700px;"
alt="two-server deployment diagram"/>
&lt;/a>
&lt;h2 id="hardware-setup">Hardware Setup&lt;/h2>
&lt;p>The JACS backend consists of several services which need to be deployed on server hardware. We have tested the following configuration:&lt;/p>
&lt;ul>
&lt;li>Two Dell PowerEdge R740XD Servers
&lt;ul>
&lt;li>Each server has 40 cores (e.g. Intel Xeon Gold 6148 2.4G)&lt;/li>
&lt;li>Each server has 192 GB of memory&lt;/li>
&lt;li>The hard drives are configured as follows:
&lt;ul>
&lt;li>2 x 200GB SSD in RAID1 - Operating system (/)&lt;/li>
&lt;li>2 x 960GB SSD in RAID1 - Databases, user preferences, etc. (/opt)&lt;/li>
&lt;li>12 x 10TB in RAID6 - Image files (/data)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>The rest of this guide assumes that you have two server hosts dedicated to deploying this system, which are configured as listed above. They will be referred to as &lt;strong>HOST1&lt;/strong> and &lt;strong>HOST2&lt;/strong>.&lt;/p>
&lt;p>This two-server deployment can support 5-10 concurrent users. We use the following configuration for client machines:&lt;/p>
&lt;ul>
&lt;li>Dell Precision 5820 Tower
&lt;ul>
&lt;li>Minimum of 8 cores (e.g. Intel Xeon W-2145 3.7GHz)&lt;/li>
&lt;li>128 GB of memory&lt;/li>
&lt;li>Nvidia GTX1080Ti 11GB (reference card, blower fan style)
&lt;ul>
&lt;li>Other similar cards will work fine: GTX1070, GTX1080, RTX2080&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Windows 10&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="install-oracle-linux-8">Install Oracle Linux 8&lt;/h2>
&lt;p>The backend software runs on any operating system which supports Docker. However, Oracle Linux is used at Janelia and has been extensively tested with this software. Therefore, we recommend installing the latest version of Oracle Linux 8.&lt;/p>
&lt;h2 id="install-docker">Install Docker&lt;/h2>
&lt;p>To install Docker and Docker Compose on Oracle Linux 8, follow &lt;a href="../installingdocker">these instructions&lt;/a>.&lt;/p>
&lt;h2 id="setup-docker-swarm">Setup Docker Swarm&lt;/h2>
&lt;p>On &lt;strong>HOST1&lt;/strong>, bring up swarm as a manager node:&lt;/p>
&lt;pre>&lt;code>docker swarm init
&lt;/code>&lt;/pre>
&lt;p>On &lt;strong>HOST2&lt;/strong>, copy and paste the output of the previous command to join the swarm as a worker.&lt;/p>
&lt;pre>&lt;code>docker swarm join --token ...
&lt;/code>&lt;/pre>
&lt;p>All further commands should be executed on &lt;strong>HOST1&lt;/strong>, i.e. the master node. One final step is to label the nodes. Each node needs the &amp;ldquo;jacs=true&amp;rdquo; label, as well as &amp;ldquo;jacs_name=nodeX&amp;rdquo;.&lt;/p>
&lt;pre>&lt;code>docker node update --label-add jacs_name=node1 $(docker node ls -f &amp;quot;role=manager&amp;quot; --format &amp;quot;{{.ID}}&amp;quot;)
docker node update --label-add jacs_name=node2 $(docker node ls -f &amp;quot;role=worker&amp;quot; --format &amp;quot;{{.ID}}&amp;quot;)
docker node update --label-add jacs=true $(docker node ls -f &amp;quot;role=manager&amp;quot; --format &amp;quot;{{.ID}}&amp;quot;)
docker node update --label-add jacs=true $(docker node ls -f &amp;quot;role=worker&amp;quot; --format &amp;quot;{{.ID}}&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>Finally, you can run this command to ensure that both nodes are up and in Ready status:&lt;/p>
&lt;pre>&lt;code>docker node ls
&lt;/code>&lt;/pre>
&lt;h2 id="download-the-installer">Download the installer&lt;/h2>
&lt;p>Download the installer and extract it onto the master node, as follows. &lt;code>VERSION&lt;/code> should be set to the &lt;a href="https://github.com/JaneliaSciComp/jacs-cm/releases">latest stable version&lt;/a> available on the releases page.&lt;/p>
&lt;pre>&lt;code>export VERSION=&amp;lt;version_number_here&amp;gt;
cd /opt
sudo mkdir deploy
sudo chown $USER deploy
cd deploy
curl https://codeload.github.com/JaneliaSciComp/jacs-cm/tar.gz/$VERSION | tar xvz
ln -s jacs-cm-$VERSION jacs-cm
cd jacs-cm
&lt;/code>&lt;/pre>
&lt;h2 id="configure-the-system">Configure The System&lt;/h2>
&lt;p>Next, create a &lt;code>.env.config&lt;/code> file inside the intaller directory. This file defines the environment (usernames, passwords, etc.) You can copy the template to get started:&lt;/p>
&lt;pre>&lt;code>cp .env.template .env.config
vi .env.config
&lt;/code>&lt;/pre>
&lt;p>At minimum, you must customize the following:&lt;/p>
&lt;ol>
&lt;li>Set &lt;code>DEPLOYMENT&lt;/code> to &lt;strong>mouselight&lt;/strong>.&lt;/li>
&lt;li>Ensure that &lt;code>REDUNDANT_STORAGE&lt;/code> and &lt;code>NON_REDUNDANT_STORAGE&lt;/code> point to the disk mounts you used during the operating system installation. Alternatively, you can make symbolic links so that the default paths point to your mounted disks.&lt;/li>
&lt;li>Set &lt;code>HOST1&lt;/code> and &lt;code>HOST2&lt;/code> to the two servers you are deploying on. Use fully-qualified hostnames here &amp;ndash; they should match the SSL certificate you intend to use.&lt;/li>
&lt;li>Fill in all the unset passwords with &amp;gt;8 character passwords. You should only use alphanumeric characters, special characters are not currently supported.&lt;/li>
&lt;li>Generate 32-byte secret keys for JWT_SECRET_KEY, MONGODB_SECRET_KEY, JACS_API_KEY, and JADE_API_KEY.&lt;/li>
&lt;li>If you want to enable automated error reporting from the Workstation client, set &lt;code>MAIL_SERVER&lt;/code> to an SMTP server and port, e.g. smtp.my.org:25.&lt;/li>
&lt;/ol>
&lt;h2 id="deploy-services">Deploy Services&lt;/h2>
&lt;p>Now you can follow the &lt;a href="../swarmdeployment">Swarm Deployment instructions&lt;/a> to actually deploy the software.&lt;/p>
&lt;h2 id="import-imagery">Import Imagery&lt;/h2>
&lt;p>If you have your own imagery, you will need to &lt;a href="../dataimport">convert it&lt;/a> before importing.&lt;/p>
&lt;p>Each brain image is referred to as a &amp;ldquo;sample&amp;rdquo;. You should place each sample in &lt;code>$DATA_DIR/jacsstorage/samples&lt;/code> on one of the servers. If you place the sample on the first server, in &lt;code>$DATA_DIR/jacsstorage/samples/&amp;lt;sampleDirectoryName&amp;gt;&lt;/code>, then in the Workstation you will refer to the sample as &lt;code>/jade1/&amp;lt;sampleDirectoryName&amp;gt;&lt;/code>.&lt;/p>
&lt;p>As a side note if you use &amp;rsquo;lvDataImport&amp;rsquo; service to generate the imagery, the service does not use JADE to persist the data. So if you need the data to be on a storage that is only accessible on certain hosts, JACS must run on that host in order to be able to write the data to the corresponding location. If that is not an option you can generate the data to a temporary location then move it to the intended sample directory.&lt;/p>
&lt;p>In the Workstation, select &lt;strong>File&lt;/strong> → &lt;strong>New&lt;/strong> → &lt;strong>Tiled Microscope Sample&lt;/strong>, and then set &amp;ldquo;Sample Name&amp;rdquo; to &lt;code>&amp;lt;sampleDirectoryName&amp;gt;&lt;/code> and &amp;ldquo;Path to Render Folder&amp;rdquo; as &lt;code>/jade1/&amp;lt;sampleDirectoryName&amp;gt;&lt;/code>.&lt;/p>
&lt;p>Open the Data Explorer (&lt;strong>Window&lt;/strong> → &lt;strong>Core&lt;/strong> → &lt;strong>Data Explorer&lt;/strong>) and navigate to Home, then &amp;ldquo;3D RawTile Microscope Samples&amp;rdquo;, and your sample name. Right-click the sample and choose &amp;ldquo;Open in Large Volume Viewer&amp;rdquo;. The 2D imagery should load into the middle panel. You should be able to right-click anywhere on the image and select &amp;ldquo;Navigate to This Location in Horta (channel 1)&amp;rdquo;, to load the 3D imagery.&lt;/p>
&lt;h2 id="find-more-information">Find More Information&lt;/h2>
&lt;p>This concludes the MouseLight Workstation installation procedure. Further information on using the tools can be found in the &lt;a href="../../../user_manual">User Manual&lt;/a>.&lt;/p></description></item><item><title>Docs: Three-server deployment</title><link>https://hortacloud.janelia.org/docs/administration/bare_metal/fulldeployment/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hortacloud.janelia.org/docs/administration/bare_metal/fulldeployment/</guid><description>
&lt;p>This document describes the full three-server Janelia Workstation deployment for supporting both FlyLight and MouseLight at Janelia Research Campus. This deployment uses Docker Swarm to orchestrate containers available on Docker Hub.&lt;/p>
&lt;p>Multiple &lt;em>environments&lt;/em> are supported with this deployment:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>prod&lt;/strong>&lt;/li>
&lt;li>&lt;strong>dev&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h2 id="deployment-diagram">Deployment Diagram&lt;/h2>
&lt;a href="../FullDeployment.png">
&lt;img
src="../FullDeployment.png"
class="rounded mx-auto"
style="max-width: 700px;"
alt="three-server deployment diagram"/>
&lt;/a>
&lt;h2 id="hardware-setup">Hardware Setup&lt;/h2>
&lt;p>This guide assumes that you have three high-end servers which can be dedicated to running Docker Swarm. We use 40-core servers with at least 192 GB of RAM. YMMV.&lt;/p>
&lt;p>We&amp;rsquo;ll refer to the three deployment hosts as &lt;strong>HOST1&lt;/strong>, &lt;strong>HOST2&lt;/strong>, and &lt;strong>HOST3&lt;/strong>.&lt;/p>
&lt;p>Note that an additional server or VM is necessary to run the JACS Async Services outside of Docker, if you are planning to submit image processing jobs to an HPC cluster, such as with the Image Processing Pipeline (IPP).&lt;/p>
&lt;h2 id="install-oracle-linux-8">Install Oracle Linux 8&lt;/h2>
&lt;p>The backend software should run on any operating system which supports Docker. However, Oracle Linux is used at Janelia and has been extensively tested with this software. Therefore, we recommend installing the latest version of Oracle Linux 8. Previously, we used Scientific Linux 7 and that is also known well although it&amp;rsquo;s no longer supported.&lt;/p>
&lt;h2 id="install-docker">Install Docker&lt;/h2>
&lt;p>To install Docker and Docker Compose on Oracle Linux 8, follow &lt;a href="../installingdockerol8">these instructions&lt;/a>.&lt;/p>
&lt;h2 id="setup-docker-swarm">Setup Docker Swarm&lt;/h2>
&lt;p>On &lt;strong>HOST1&lt;/strong>, bring up swarm as a manager node, and give it a label:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker swarm init
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>On &lt;strong>HOST2&lt;/strong> and &lt;strong>HOST3&lt;/strong>, copy and paste the output of the previous command to join the swarm as a worker.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker swarm join --token ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>All further commands should be executed on &lt;strong>HOST1&lt;/strong>, i.e. the master node. One final step is to label the nodes. Each node needs the &amp;ldquo;jacs=true&amp;rdquo; label, as well as &amp;ldquo;jacs_name=nodeX&amp;rdquo;. You can find out the node ids by running &lt;code>docker node ls&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>docker node update --label-add &lt;span style="color:#000">jacs_name&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>node1 &amp;lt;id of HOST1&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker node update --label-add &lt;span style="color:#000">jacs_name&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>node2 &amp;lt;id of HOST2&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker node update --label-add &lt;span style="color:#000">jacs_name&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>node3 &amp;lt;id of HOST3&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker node update --label-add &lt;span style="color:#000">jacs&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#204a87">true&lt;/span> &amp;lt;id of HOST1&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker node update --label-add &lt;span style="color:#000">jacs&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#204a87">true&lt;/span> &amp;lt;id of HOST2&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>docker node update --label-add &lt;span style="color:#000">jacs&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#204a87">true&lt;/span> &amp;lt;id of HOST3&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="download-the-installer">Download the installer&lt;/h2>
&lt;p>Download the installer and extract it onto the master node, as follows. &lt;code>VERSION&lt;/code> should be set to the &lt;a href="https://github.com/JaneliaSciComp/jacs-cm/releases">latest stable version&lt;/a> available on the releases page.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">export&lt;/span> &lt;span style="color:#000">VERSION&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&amp;lt;version_number_here&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> /opt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sudo mkdir deploy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sudo chown &lt;span style="color:#000">$USER&lt;/span> deploy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> deploy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl https://codeload.github.com/JaneliaSciComp/jacs-cm/tar.gz/&lt;span style="color:#000">$VERSION&lt;/span> &lt;span style="color:#000;font-weight:bold">|&lt;/span> tar xvz
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ln -s jacs-cm-&lt;span style="color:#000">$VERSION&lt;/span> jacs-cm
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87">cd&lt;/span> jacs-cm
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="configure-the-system">Configure The System&lt;/h2>
&lt;p>Next, create a &lt;code>.env.config&lt;/code> file inside the installer directory. This file defines the environment (usernames, passwords, etc.) You can copy the template to get started:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cp .env.template .env.config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>vi .env.config
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>At minimum, you must customize the following:&lt;/p>
&lt;ol>
&lt;li>Ensure that &lt;code>REDUNDANT_STORAGE&lt;/code> and &lt;code>NON_REDUNDANT_STORAGE&lt;/code> point to the disk mounts available on the local systems. Alternatively, you can make symbolic links so that the default paths point to your mounted disks.&lt;/li>
&lt;li>Set &lt;code>HOST1&lt;/code>, &lt;code>HOST2&lt;/code>, and &lt;code>HOST3&lt;/code> to the servers you are deploying on. Use fully-qualified hostnames here &amp;ndash; they should match the SSL certificate you intend to use.&lt;/li>
&lt;li>Fill in all the unset passwords with &amp;gt;8 character passwords. You should only use alphanumeric characters, special characters are not currently supported.&lt;/li>
&lt;li>Generate 32-byte secret keys for JWT_SECRET_KEY and MONGODB_SECRET_KEY.&lt;/li>
&lt;/ol>
&lt;h2 id="enable-databases-optional">Enable Databases (optional)&lt;/h2>
&lt;p>Currently, Janelia runs MongoDB outside of the Swarm, so they are commented out in the deployment. If you&amp;rsquo;d like to run the databases as part of the swarm, edit the yaml files under ./deployments/jacs/ and uncomment the databases.&lt;/p>
&lt;h2 id="deploy-services">Deploy Services&lt;/h2>
&lt;p>Now you can follow the &lt;a href="../swarmdeployment">Swarm Deployment instructions&lt;/a> to actually deploy the software.&lt;/p>
&lt;h2 id="deploy-elk-for-monitoring">Deploy ELK for monitoring&lt;/h2>
&lt;p>To deploy an ELK stack for monitoring follow &lt;a href="../elkdeployment">ELK Deployment&lt;/a>.&lt;/p>
&lt;h2 id="find-more-information">Find More Information&lt;/h2>
&lt;p>This concludes the MouseLight Workstation installation procedure. Further information on using the tools can be found in the &lt;a href="../../../user_manual">User Manual&lt;/a>.&lt;/p></description></item><item><title>Docs: Installing Docker</title><link>https://hortacloud.janelia.org/docs/administration/bare_metal/installingdocker/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hortacloud.janelia.org/docs/administration/bare_metal/installingdocker/</guid><description>
&lt;p>To install Docker on a server running Oracle Linux 8, some special configuration is needed. Much of this comes from the &lt;a href="https://docs.docker.com/install/linux/docker-ce/centos/">official documentation&lt;/a>.&lt;/p>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;p>First, make sure that Docker isn’t already installed:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>yum list installed &lt;span style="color:#000;font-weight:bold">|&lt;/span> grep docker
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Remove any existing installations before proceeding.&lt;/p>
&lt;p>Ensure that /opt (or whatever disk is to be used for Docker data) is formatted with the d_type option. You can find out like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ xfs_info /opt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>meta-data&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>/dev/mapper/vg0-lv_opt &lt;span style="color:#000">isize&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">512&lt;/span> &lt;span style="color:#000">agcount&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>4, &lt;span style="color:#000">agsize&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">5701632&lt;/span> &lt;span style="color:#000">blks&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">sectsz&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">4096&lt;/span> &lt;span style="color:#000">attr&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>2, &lt;span style="color:#000">projid32bit&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#000">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">crc&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">1&lt;/span> &lt;span style="color:#000">finobt&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> &lt;span style="color:#000">spinodes&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">data&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">bsize&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">4096&lt;/span> &lt;span style="color:#000">blocks&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>22806528, &lt;span style="color:#000">imaxpct&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#000">25&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">sunit&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> &lt;span style="color:#000">swidth&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> blks
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">naming&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>version &lt;span style="color:#0000cf;font-weight:bold">2&lt;/span> &lt;span style="color:#000">bsize&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">4096&lt;/span> ascii-ci&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> &lt;span style="color:#000">ftype&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">log&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>internal &lt;span style="color:#000">bsize&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">4096&lt;/span> &lt;span style="color:#000">blocks&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>11136, &lt;span style="color:#000">version&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#000">2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span> &lt;span style="color:#000">sectsz&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">4096&lt;/span> &lt;span style="color:#000">sunit&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">1&lt;/span> blks, lazy-count&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">realtime&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>none &lt;span style="color:#000">extsz&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">4096&lt;/span> &lt;span style="color:#000">blocks&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>0, &lt;span style="color:#000">rtextents&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#0000cf;font-weight:bold">0&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If the above says ftype=0, then the filesystem will need to be recreated (&lt;a href="https://linuxer.pro/2017/03/what-is-d_type-and-why-docker-overlayfs-need-it/">reference&lt;/a>).&lt;/p>
&lt;h2 id="installing-docker">Installing Docker&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>sudo yum install -y yum-utils
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sudo yum install -y docker-ce
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If this fails with error messages like &lt;code>package containerd.io-1.4.3-3.2.el8.x86_64 conflicts with runc provided by runc&lt;/code> then you may have conflicting packages installed already. Try removing them like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>sudo yum erase podman buildah
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="post-install-configuration">Post Install Configuration&lt;/h2>
&lt;p>To avoid running out of space on the root partition, you should configure docker to point to /opt/docker (&lt;a href="https://www.rb-associates.co.uk/blog/move-var-lib-docker-to-another-directory/">reference&lt;/a>):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>sudo mkdir -p /opt/docker
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sudo chown root:root /opt/docker
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sudo chmod &lt;span style="color:#0000cf;font-weight:bold">701&lt;/span> /opt/docker
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next, configure Docker to use the overlay2 storage driver (&lt;a href="https://www.projectatomic.io/blog/2015/06/notes-on-fedora-centos-and-docker-storage-drivers/">reference&lt;/a>).&lt;/p>
&lt;p>Create a file at /etc/docker/daemon.json with this content:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">{&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">&amp;#34;data-root&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;/opt/docker&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#204a87;font-weight:bold">&amp;#34;storage-driver&amp;#34;&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span> &lt;span style="color:#4e9a06">&amp;#34;overlay2&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000;font-weight:bold">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You should also create a local user called &amp;ldquo;docker-nobody&amp;rdquo; with UID 4444, which can be used for running containers without root.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>sudo groupadd -g &lt;span style="color:#0000cf;font-weight:bold">4444&lt;/span> docker-nobody
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sudo useradd --uid &lt;span style="color:#0000cf;font-weight:bold">4444&lt;/span> --gid &lt;span style="color:#0000cf;font-weight:bold">4444&lt;/span> --shell /sbin/nologin docker-nobody
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Finally, you can start Docker:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>sudo systemctl &lt;span style="color:#204a87">enable&lt;/span> docker
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sudo systemctl start docker
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="installing-docker-compose">Installing Docker Compose&lt;/h2>
&lt;p>You&amp;rsquo;ll also need to install the Docker Compose executable:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>sudo curl -L &lt;span style="color:#4e9a06">&amp;#34;https://github.com/docker/compose/releases/download/1.23.2/docker-compose-&lt;/span>&lt;span style="color:#204a87;font-weight:bold">$(&lt;/span>uname -s&lt;span style="color:#204a87;font-weight:bold">)&lt;/span>&lt;span style="color:#4e9a06">-&lt;/span>&lt;span style="color:#204a87;font-weight:bold">$(&lt;/span>uname -m&lt;span style="color:#204a87;font-weight:bold">)&lt;/span>&lt;span style="color:#4e9a06">&amp;#34;&lt;/span> -o /usr/local/bin/docker-compose
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sudo chmod +x /usr/local/bin/docker-compose
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that there are newer versions of the Docker Compose, but they have bugs that prevent them from working with our scripts. Please use the version above to ensure compatibility.&lt;/p></description></item><item><title>Docs: Deploying JACS</title><link>https://hortacloud.janelia.org/docs/administration/bare_metal/swarmdeployment/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hortacloud.janelia.org/docs/administration/bare_metal/swarmdeployment/</guid><description>
&lt;p>This document assumes that you have downloaded and configured the installer according to one of the deployment guides.&lt;/p>
&lt;p>The following steps are common to all Docker Swarm deployments of the Workstation.&lt;/p>
&lt;h2 id="initialize-filesystems">Initialize Filesystems&lt;/h2>
&lt;p>The first step is to initialize the filesystems on all your Swarm systems. On each server, ensure that your &lt;code>REDUNDANT_STORAGE&lt;/code> (default: /opt/jacs), &lt;code>NON_REDUNDANT_STORAGE&lt;/code> (default: /data) directories exist and can be written to by your UNAME:GNAME user (default: docker-nobody).&lt;/p>
&lt;p>Then, run the Swarm-based initialization procedure from &lt;strong>HOST1&lt;/strong>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>./manage.sh init-filesystems
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can manually edit the files found in &lt;code>CONFIG_DIR&lt;/code> to further customize the installation.&lt;/p>
&lt;p>Once the initialization completes you can just run:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>./manage.sh stop
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Also it is a good idea to stop the initialization stack if anything goes wrong before you try it again.&lt;/p>
&lt;h3 id="ssl-certificates">SSL Certificates&lt;/h3>
&lt;p>At this point, &lt;strong>it is strongly recommended is to replace the self-signed certificates&lt;/strong> in &lt;code>$CONFIG_DIR/certs/*&lt;/code> on each server with your own certificates signed by a Certificate Authority:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>. .env
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sudo cp /path/to/your/certs/cert.&lt;span style="color:#ce5c00;font-weight:bold">{&lt;/span>crt,key&lt;span style="color:#ce5c00;font-weight:bold">}&lt;/span> &lt;span style="color:#000">$CONFIG_DIR&lt;/span>/certs/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sudo chown docker-nobody:docker-nobody &lt;span style="color:#000">$CONFIG_DIR&lt;/span>/certs/*
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you continue with the self-signed certificates, you will need to &lt;a href="../selfsignedcerts">set up the trust chain&lt;/a> for them later.&lt;/p>
&lt;h3 id="external-authentication">External Authentication&lt;/h3>
&lt;p>The JACS system has its own self-contained authentication system, and can manage users and passwords internally.&lt;/p>
&lt;p>If you&amp;rsquo;d prefer that users authenticate against your existing LDAP or ActiveDirectory server, edit &lt;code>$CONFIG_DIR/jacs-sync/jacs.properties&lt;/code> and add these properties:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>LDAP.URL&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>LDAP.SearchBase&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>LDAP.SearchFilter&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>LDAP.BindDN&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>LDAP.BindCredentials&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The URL should point to your authentication server. The SearchBase is part of a distinguished name to search, something like &amp;ldquo;ou=People,dc=yourorg,dc=org&amp;rdquo;. The SearchFilter is the attribute to search on, something like &amp;ldquo;(cn={{username}})&amp;rdquo;. BindDN and BindCredentials defines the distinguished name and password for a service user that can access user information like full names and emails.&lt;/p>
&lt;h2 id="start-all-containers">Start All Containers&lt;/h2>
&lt;p>Next, start up all of the service containers. The parameter to the start command specifies the environment to use. The &lt;strong>dev&lt;/strong> environment uses containers tagged as &lt;em>latest&lt;/em> and updates them automatically when they change. The &lt;strong>prod&lt;/strong> deployment uses a frozen set of production versions. When in doubt, use the &lt;strong>prod&lt;/strong> deployment:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>./manage.sh start
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>It may take a minute for the containers to spin up. You can monitor the progress with this command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>./manage.sh status
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>At this stage, some of the services may not start because they depend on the databases. The next step will take care of that.&lt;/p>
&lt;h2 id="initialize-databases">Initialize Databases&lt;/h2>
&lt;p>Now you are ready to initalize the databases:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>./manage.sh init-databases
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>It&amp;rsquo;s normal to see the &amp;ldquo;Unable to reach primary for set rsJacs&amp;rdquo; error repeated until the Mongo replica set converges on healthiness. After a few seconds, you should see a message &amp;ldquo;Databases have been initialized&amp;rdquo; and the process will exit successfully.&lt;/p>
&lt;p>You can validate the databases as follows:&lt;/p>
&lt;ul>
&lt;li>Verify that you can connect to the Mongo instance using &lt;code>./manage.sh mongo&lt;/code>, and run &lt;code>show tables&lt;/code>&lt;/li>
&lt;li>Connect to the RabbitMQ server at http://&lt;strong>HOST1&lt;/strong>:15672 and log in with your &lt;code>RABBITMQ_USER&lt;/code>/&lt;code>RABBITMQ_PASSWORD&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="restart-services">Restart Services&lt;/h2>
&lt;p>Bounce the stack so that everything reconnects to the databases:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>./manage.sh stop
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>./manage.sh start
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now you shoult wait for all the services to start. You can continue to monitor the progress with this command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>./manage.sh status
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If any container failed to start up, it will show up with &amp;ldquo;0/N&amp;rdquo; replicas, and it will need to be investigated before moving further. You can view the corresponding error by specifying the swarm service name, as reported by the status command. For example, if jacs_jade-agent2 fails to start, you would type:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>./manage.sh status jacs_jade-agent2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="verify-functionality">Verify Functionality&lt;/h2>
&lt;p>You can verify the Authentication Service is working as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>./manage.sh login
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You should be able to log in with the default admin account (root/root), or any LDAP/AD account if you&amp;rsquo;ve configured external authentication. This will return a JWT that can be used on subsequent requests.&lt;/p>
&lt;p>If you run into any problems, these &lt;a href="../troubleshooting">troubleshooting tips&lt;/a> may help.&lt;/p>
&lt;h2 id="manage-services">Manage Services&lt;/h2>
&lt;p>As long as your Docker daemon is configured to restart on boot, all of the Swarm services will also restart automatically when the server is rebooted.&lt;/p>
&lt;p>If you want to remove all the services from the Swarm and do a clean restart of everything, you can use this command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>./manage.sh stop
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>To pull and redeploy the latest image for a single service, e.g. workstation-site:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>./manage.sh restart jacs_workstation-site
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="configure-crontabs">Configure Crontabs&lt;/h2>
&lt;p>The following crontab entries should be configured in order to perform periodic maintenance automatically. It&amp;rsquo;s easiest to install the crontabs on the docker-nobody account:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>sudo crontab -u docker-nobody -e
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Database maintenance refreshes indexes and updates entities permissions:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">2&lt;/span> * * * /opt/deploy/jacs-cm/manage.sh dbMaintenance group:admin -refreshIndexes -refreshPermissions
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>SOLR index refresh (if using SOLR):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">3&lt;/span> * * * /opt/deploy/jacs-cm/manage.sh rebuildSolrIndex
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Database backups (if using containerized databases):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#0000cf;font-weight:bold">0&lt;/span> &lt;span style="color:#0000cf;font-weight:bold">4&lt;/span> * * * /opt/deploy/jacs-cm/manage.sh backup mongo
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="install-the-workstation-client">Install The Workstation Client&lt;/h2>
&lt;p>Now that the services are all running, you can navigate to &lt;a href="https://HOST1">https://HOST1&lt;/a> in a web browser on your client machine, and download the Workstation client. Follow the installer wizard, using the default options, then launch the Workstation.&lt;/p>
&lt;p>If you are using the default self-signed certificate, you will need to take some extra steps to &lt;a href="../selfsignedcerts">install it on the client&lt;/a>.&lt;/p>
&lt;p>If you are using LDAP/AD integration, you should be able to log in with your normal user/password. If you are using the Workstation&amp;rsquo;s internal user management, you must first login as user root (password: root), and then select &lt;strong>Window&lt;/strong> → &lt;strong>Core&lt;/strong> → &lt;strong>Administrative GUI&lt;/strong> from the menus. Click &amp;ldquo;View Users&amp;rdquo;, then &amp;ldquo;New User&amp;rdquo; and create your first user. Add the user to all of the relevant groups, including MouseLight.&lt;/p>
&lt;h2 id="optional-adding-nfs-storage">Optional: Adding NFS Storage&lt;/h2>
&lt;p>If you have data on NFS, and those NFS drives can be mounted on the MouseLight hosts, that data can be made available to the Workstation.&lt;/p>
&lt;p>First, create a file at deployments/mouselight/docker-swarm.prod.yml which looks like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">version&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#39;3.7&amp;#39;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline">&lt;/span>&lt;span style="color:#204a87;font-weight:bold">services&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">jade-agent1&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">volumes&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>- &lt;span style="color:#000">/path/to/your/nfs:/path/to/your/nfs:ro,shared&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">jade-agent2&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">volumes&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>- &lt;span style="color:#000">/path/to/your/nfs:/path/to/your/nfs:ro,shared&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will expose the path to both JADE agent containers. Now you need to configure the JADE agents to serve this data. On both hosts, edit /opt/jacs/config/jade/config.properties and add the following:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>StorageVolume.mouseLightNFS.RootDir&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>/path/to/your/nfs
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>StorageVolume.mouseLightNFS.VirtualPath&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>/path/to/your/nfs
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>StorageVolume.mouseLightNFS.Shared&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#204a87">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>StorageVolume.mouseLightNFS.Tags&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>mousebrain,light
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>StorageVolume.mouseLightNFS.VolumePermissions&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>READ
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can use any name you want instead of mouseLightNFS. Then you should add this name to StorageAgent.BootstrappedVolumes:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>StorageAgent.BootstrappedVolumes&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>jade1,mouseLightNFS
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You will need to bounce the service stack to pick up these changes.&lt;/p></description></item><item><title>Docs: Deploying ELK</title><link>https://hortacloud.janelia.org/docs/administration/bare_metal/elkdeployment/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hortacloud.janelia.org/docs/administration/bare_metal/elkdeployment/</guid><description>
&lt;p>Currently ELK is only available in a swarm deployment because of how the ELK stack is configured.&lt;/p>
&lt;p>Most of the set up necessary for ELK - configuration files and/or data directories - is already done as part of filesystem initialization performed while executing:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>./manage.sh init-filesystems
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The current ELK data directories are created on &lt;code>/data/&lt;/code> directory so this directory must exist.&lt;/p>
&lt;p>One manual step is to set the &lt;code>vm.max_map_count&lt;/code> value required for running elasticsearch. This is done by adding &lt;code>vm.max_map_count=262144&lt;/code> line to &lt;code>/etc/sysctl.conf&lt;/code> and then run &lt;code>sysctl -p&lt;/code> (typically this must be done as root)&lt;/p>
&lt;p>After that deploying the elk stack for monitoring the application only requires starting with:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>./manage.sh start-elk
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The command to stop the monitoring is:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>./manage.sh stop-elk
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="import-data-from-an-old-stack">Import data from an old stack&lt;/h2>
&lt;p>To import data from an old stack the old stack nodes must be whitelisted in the &lt;code>ELK_WHITELIST&lt;/code> environment variable so that they can be accessible to the current cluster for importing indexes.&lt;/p>
&lt;p>Some useful elasticsearch endpoints:&lt;/p>
&lt;ul>
&lt;li>List available indices&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>curl http://&amp;lt;escoordinator&amp;gt;:9200/_cat/indices
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>Import an index from a remote cluster:&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic">#!/bin/bash
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic">&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">remoteost&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#000">$1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000">indexName&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>&lt;span style="color:#000">$2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl -H &lt;span style="color:#4e9a06">&amp;#39;Content-Type: application/json&amp;#39;&lt;/span> -X POST http://e03u08.int.janelia.org:9200/_reindex -d &lt;span style="color:#4e9a06">&amp;#34;{
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> \&amp;#34;source\&amp;#34;: {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> \&amp;#34;remote\&amp;#34;: {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> \&amp;#34;host\&amp;#34;: \&amp;#34;http://&lt;/span>&lt;span style="color:#4e9a06">${&lt;/span>&lt;span style="color:#000">remoteHost&lt;/span>&lt;span style="color:#4e9a06">}&lt;/span>&lt;span style="color:#4e9a06">:9200\&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> },
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> \&amp;#34;index\&amp;#34;: \&amp;#34;&lt;/span>&lt;span style="color:#4e9a06">${&lt;/span>&lt;span style="color:#000">indexName&lt;/span>&lt;span style="color:#4e9a06">}&lt;/span>&lt;span style="color:#4e9a06">\&amp;#34;,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> \&amp;#34;query\&amp;#34;: {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> \&amp;#34;match_all\&amp;#34;: {}
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> }
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> },
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> \&amp;#34;dest\&amp;#34;: {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> \&amp;#34;index\&amp;#34;: \&amp;#34;&lt;/span>&lt;span style="color:#4e9a06">${&lt;/span>&lt;span style="color:#000">indexName&lt;/span>&lt;span style="color:#4e9a06">}&lt;/span>&lt;span style="color:#4e9a06">\&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> }
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">}&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>Export kibana objects&lt;/li>
&lt;/ul>
&lt;p>Here&amp;rsquo;s an example to export kibana visualizations but the command is identical for any one of [config, index-pattern, visualization, search, dashboard, url] - just set the appropriate type&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>curl http://&amp;lt;oldkibanahost&amp;gt;:5601/api/saved_objects/_export -H &lt;span style="color:#4e9a06">&amp;#39;kbn-xsrf: true&amp;#39;&lt;/span> &lt;span style="color:#4e9a06">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&lt;/span> -H &lt;span style="color:#4e9a06">&amp;#39;Content-Type: application/json&amp;#39;&lt;/span> &lt;span style="color:#4e9a06">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&lt;/span> -d &lt;span style="color:#4e9a06">&amp;#39;{&amp;#34;type&amp;#34;: &amp;#34;visualization&amp;#34; }&amp;#39;&lt;/span> &amp;gt; local/kibana-visualizations.ndjson
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>To import&lt;/li>
&lt;/ul>
&lt;p>Use the file generated by the above export command and run&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>curl -X POST http://e03u06.int.janelia.org:5601/api/saved_objects/_import &lt;span style="color:#4e9a06">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&lt;/span> -H &lt;span style="color:#4e9a06">&amp;#39;kbn-xsrf: true&amp;#39;&lt;/span> &lt;span style="color:#4e9a06">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&lt;/span> --form &lt;span style="color:#000">file&lt;/span>&lt;span style="color:#ce5c00;font-weight:bold">=&lt;/span>@local/kibana-visualizations.ndjson
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Self-signed Certificates</title><link>https://hortacloud.janelia.org/docs/administration/bare_metal/selfsignedcerts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hortacloud.janelia.org/docs/administration/bare_metal/selfsignedcerts/</guid><description>
&lt;p>A self-signed certificate is automatically generated during the &lt;code>init-filesystem&lt;/code> step of a jacs-cm installation. For production use, it is recommended that you replace this certificate with a real one. The self-signed certificate is less secure, and it requires some extra steps to get working.&lt;/p>
&lt;p>In order to connect to &lt;a href="https://HOST1">https://HOST1&lt;/a>, you need to accept the certificate in the browser. This differs by browser.&lt;/p>
&lt;p>Then, in order to allow the Workstation to accept the certificate, it needs to be added to Java&amp;rsquo;s keystore. For this, you will need the certificate on the desktop computer where you are running the Workstation. You can either export it from the browser, or copy it over from the server. On the server, it is located in &lt;code>$CONFIG_DIR/certs/cert.crt&lt;/code>. Once you have the certificate, you can import it using Java&amp;rsquo;s keytool.&lt;/p>
&lt;h2 id="windows">Windows&lt;/h2>
&lt;p>On Windows, click Start and type &amp;ldquo;cmd&amp;rdquo; to find the Command Prompt, then right-click it and select &amp;ldquo;Run as administrator&amp;rdquo;. You need to find out where your JVM is installed by looking under C:\Program Files\Zulu. Then, import the certificate. Here it&amp;rsquo;s assumed the cert was saved to the current working directory:&lt;/p>
&lt;pre tabindex="0">&lt;code>C:\&amp;gt; &amp;#34;C:\Program Files\Zulu\zulu-8\bin\keytool.exe&amp;#34; -import -alias mouse1selfcert -file cert.crt -keystore &amp;#34;C:\Program Files\Zulu\zulu-8\jre\lib\security\cacerts&amp;#34; -keypass changeit -storepass changeit
&lt;/code>&lt;/pre>&lt;p>The &lt;strong>alias&lt;/strong> should be a descriptive name that will be used later if you want to remove or view the certificate. The password for the JVM keystore is actually &amp;ldquo;changeit&amp;rdquo;, so don&amp;rsquo;t change the &lt;strong>keypass&lt;/strong> or &lt;strong>storepass&lt;/strong> values above.&lt;/p>
&lt;h2 id="mac-or-linux">Mac or Linux&lt;/h2>
&lt;p>First, you need to know where the JVM is located. You can use the same method that the Workstation uses to locate the JVM. This ensures you are modifying the correct one. Open a Terminal and type:&lt;/p>
&lt;pre tabindex="0">&lt;code>export JDK_HOME=`/usr/libexec/java_home -v 1.8`
&lt;/code>&lt;/pre>&lt;p>Now you can import the certificate into the keystore. Here it&amp;rsquo;s assumed the cert was saved to the desktop:&lt;/p>
&lt;pre tabindex="0">&lt;code>sudo keytool -import -v -trustcacerts -alias mouse1 -file ~/Desktop/cert.crt -keystore $JDK_HOME/jre/lib/security/cacerts -keypass changeit -storepass changeit
&lt;/code>&lt;/pre>&lt;p>The &lt;strong>alias&lt;/strong> should be a descriptive name that will be used later if you want to remove or view the certificate. The password for the JVM keystore is actually &amp;ldquo;changeit&amp;rdquo;, so don&amp;rsquo;t change the &lt;strong>keypass&lt;/strong> or &lt;strong>storepass&lt;/strong> values above.&lt;/p></description></item><item><title>Docs: Contribution guidelines</title><link>https://hortacloud.janelia.org/docs/development/contribution/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hortacloud.janelia.org/docs/development/contribution/</guid><description/></item><item><title>Docs: Operations</title><link>https://hortacloud.janelia.org/docs/administration/aws/operations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hortacloud.janelia.org/docs/administration/aws/operations/</guid><description>
&lt;div class="pageinfo pageinfo-primary">
&lt;p>We are in the process of writing this documentation.&lt;/p>
&lt;/div></description></item><item><title>Docs: Storage volumes</title><link>https://hortacloud.janelia.org/docs/administration/bare_metal/storagevolumes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hortacloud.janelia.org/docs/administration/bare_metal/storagevolumes/</guid><description>
&lt;p>The Workstation/JACS system relies on JADE for its storage API.&lt;/p>
&lt;h2 id="adding-a-new-storage-volume">Adding a new Storage Volume&lt;/h2>
&lt;p>&lt;strong>Add bootstrap to the JADE configuration&lt;/strong>&lt;/p>
&lt;p>On HOST1, edit /opt/jacs/config/jade/config.properties and add a block for your new volume, for example:&lt;/p>
&lt;pre tabindex="0">&lt;code>StorageVolume.s3data.RootDir=/s3data
StorageVolume.s3data.VirtualPath=/s3jade
StorageVolume.s3data.Shared=true
StorageVolume.s3data.Tags=aws,cloud
StorageVolume.s3data.VolumePermissions=READ,WRITE,DELETE
&lt;/code>&lt;/pre>&lt;p>The properties configure the volume as follows:&lt;/p>
&lt;ul>
&lt;li>RootDir: defines the actual path to the data on disk&lt;/li>
&lt;li>VirtualPath: optionally defines a virtual path which is mapped to the actual path&lt;/li>
&lt;li>Shared: true if the volume should be accessible to all volumes&lt;/li>
&lt;li>Tags: tags used by applications to find appropriate volumes&lt;/li>
&lt;li>VolumePermissions: list of operations that JADE can execute (READ,WRITE,DELETE)&lt;/li>
&lt;/ul>
&lt;p>Also add your volume to StorageAgent.BootstrappedVolumes, so that it will be created the next time the service is restarted.&lt;/p>
&lt;p>&lt;strong>Mount the path into the containers&lt;/strong>&lt;/p>
&lt;p>Edit the compose/swarm files for your deployment and mount the volume path as a Docker volume. For example, if your &lt;code>DEPLOYMENT&lt;/code> is jacs, and &lt;code>STAGE&lt;/code> is dev, you must edit these two files:&lt;/p>
&lt;p>&lt;code>deployments/jacs/docker-compose.dev.yml&lt;/code>
&lt;code>deployments/jacs/docker-swarm.dev.yml&lt;/code>&lt;/p>
&lt;p>You should add your volume for all services &lt;code>jade-agent&amp;lt;N&amp;gt;&lt;/code> which you want to service that volume.&lt;/p>
&lt;p>For example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">jade-agent1&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">volumes&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>- &lt;span style="color:#000">/data/s3data:/s3data:shared&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Restart the stack after making the changes above and the volume will be created when the JADE worker starts.&lt;/p>
&lt;h2 id="host-specific-volumes">Host-specific Volumes&lt;/h2>
&lt;p>By default, all JADE agents are configured to serve all volumes in the database. You can use &lt;code>StorageAgent.ServedVolumes&lt;/code> to control which volumes are served by which hosts.&lt;/p></description></item><item><title>Docs: Data import</title><link>https://hortacloud.janelia.org/docs/administration/bare_metal/dataimport/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hortacloud.janelia.org/docs/administration/bare_metal/dataimport/</guid><description>
&lt;p>In principle, any 3d volumetric data can be imported into the MouseLight Workstation.
At this moment we only provide some very basic tools for converting only TIFF format images into the expected format on disk. Another limitation of the current tools is that the entire volume must be in a single tiff file (per channel)&lt;/p>
&lt;p>The imagery for MouseLight Workstation is a directory containing TIFF and KTX images organized into octrees. JACS compute includes a a service that can generate the octree data from a single volume TIFF file. If there is more than 1 channel, the channels are numbered 0 .. n-1 and each channel is expected to be in its own file. For example if you have 2 channels you would have two tiff files:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>/path/to/volume/volume.0.tiff
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>/path/to/volume/volume.1.tiff
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The import service requires docker or singularity installed because the actual conversion services are packaged in two docker containers - one that generates a TIFF octree and the other one that takes the TIFF octree and converts the octant channel images into the correspomding ktx blocks.&lt;/p>
&lt;p>Currently pre-built containers for tiff octree tool and ktx tool are only available at Janelia&amp;rsquo;s internal registry, but the containers build files are available at &lt;a href="https://github.com/JaneliaSciComp/jacs-tools-docker.git">https://github.com/JaneliaSciComp/jacs-tools-docker.git&lt;/a> in the &amp;rsquo;tiff_octree&amp;rsquo; and &amp;lsquo;ktx_octree&amp;rsquo; subdirectories, respectively. KTX tool container can also be built from &lt;a href="https://github.com/JaneliaSciComp/pyktx.git">https://github.com/JaneliaSciComp/pyktx.git&lt;/a>.&lt;/p>
&lt;p>Generating the sample octree requires only a JACS Async service call which is a simple HTTP REST call that can be done using curl or Postman. This service can also be invoked from the JACS dashboard &lt;a href="http://api-gateway-host:8080">http://api-gateway-host:8080&lt;/a> by going to the &amp;ldquo;Services List&amp;rdquo; after Login and selecting &amp;ldquo;lvDataImport&amp;rdquo;. The dashboard should also offer a brief description of each argument.&lt;/p>
&lt;p>curl invocation to run the service with singularity (this is the JACS default):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>curl -X POST &lt;span style="color:#4e9a06">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&lt;/span> https://api-gateway-host/SCSW/JACS2AsyncServices/v2/async-services/lvDataImport &lt;span style="color:#4e9a06">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&lt;/span> -H &lt;span style="color:#4e9a06">&amp;#39;Accept: application/json&amp;#39;&lt;/span> &lt;span style="color:#4e9a06">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&lt;/span> -H &lt;span style="color:#4e9a06">&amp;#39;Content-Type: application/json&amp;#39;&lt;/span> &lt;span style="color:#4e9a06">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&lt;/span> -H &lt;span style="color:#4e9a06">&amp;#39;Authorization: Bearer Your_Token&amp;#39;&lt;/span> &lt;span style="color:#4e9a06">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&lt;/span> -d &lt;span style="color:#4e9a06">&amp;#39;{
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> &amp;#34;args&amp;#34;: [
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> &amp;#34;-containerProcessor&amp;#34;, &amp;#34;singularity&amp;#34;,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> &amp;#34;-inputDir&amp;#34;, &amp;#34;/path/to/volumeData&amp;#34;,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> &amp;#34;-inputFilenamePattern&amp;#34;, &amp;#34;test.{channel}.tif&amp;#34;,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> &amp;#34;-outputDir&amp;#34;, &amp;#34;/path/to/lvv/sampleData&amp;#34;,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> &amp;#34;-channels&amp;#34;, &amp;#34;0,1&amp;#34;,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> &amp;#34;-levels&amp;#34;, &amp;#34;4&amp;#34;,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> &amp;#34;-voxelSize&amp;#34;, &amp;#34;1,1,1&amp;#34;,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> &amp;#34;-subtreeLengthForSubjobSplitting&amp;#34;, 2,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> &amp;#34;-tiffOctreeContainerImage&amp;#34;, &amp;#34;docker://registry.int.janelia.org/jacs-scripts/octree:1.0&amp;#34;,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> &amp;#34;-ktxOctreeContainerImage&amp;#34;, &amp;#34;docker://registry.int.janelia.org/jacs-scripts/pyktx:1.0&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> ],
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> &amp;#34;resources&amp;#34;: {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06"> }
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">}
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#4e9a06">&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>curl invocation to run the service with docker:&lt;/p>
&lt;pre tabindex="0">&lt;code>curl -X POST \
https://api-gateway-host/SCSW/JACS2AsyncServices/v2/lvDataImport \
-H &amp;#39;Accept: application/json&amp;#39; \
-H &amp;#39;Content-Type: application/json&amp;#39; \
-H &amp;#39;Authorization: Bearer Your_Token&amp;#39; \
-d &amp;#39;{
&amp;#34;args&amp;#34;: [
&amp;#34;-containerProcessor&amp;#34;, &amp;#34;docker&amp;#34;,
&amp;#34;-inputDir&amp;#34;, &amp;#34;/path/to/volumeData&amp;#34;,
&amp;#34;-inputFilenamePattern&amp;#34;, &amp;#34;default.{channel}.tif&amp;#34;,
&amp;#34;-outputDir&amp;#34;, &amp;#34;/path/to/lvv/sampleData&amp;#34;,
&amp;#34;-channels&amp;#34;, &amp;#34;0&amp;#34;,
&amp;#34;-levels&amp;#34;, &amp;#34;3&amp;#34;,
&amp;#34;-voxelSize&amp;#34;, &amp;#34;1,1,1&amp;#34;,
&amp;#34;-subtreeLengthForSubjobSplitting&amp;#34;, 3,
&amp;#34;-tiffOctreeContainerImage&amp;#34;, &amp;#34;registry.int.janelia.org/jacs-scripts/octree:1.0&amp;#34;,
&amp;#34;-ktxOctreeContainerImage&amp;#34;, &amp;#34;registry.int.janelia.org/jacs-scripts/pyktx:1.0&amp;#34;
],
&amp;#34;resources&amp;#34;: {
}
}
&amp;#39;
&lt;/code>&lt;/pre>&lt;h3 id="arguments-description">Arguments description&lt;/h3>
&lt;ul>
&lt;li>containerProcessor - which container runtime to use docker or singularity&lt;/li>
&lt;li>inputDir - path to original volume data&lt;/li>
&lt;li>inputFileNamePattern - original tiff name. Notice that if you have multiple channels and the channel is anywhere in the name you can use &lt;code>{channel}&lt;/code> which will be replaced with the actual channel number.&lt;/li>
&lt;li>outputDir - where the octree will be generated - typically this is the sample data directory that will be imported in the workstation&lt;/li>
&lt;li>channels - specifies a list of all available channels, e.g. &amp;lsquo;0,1&amp;rsquo; if there are two channels or &amp;lsquo;0&amp;rsquo; if there is only 1 channel.&lt;/li>
&lt;li>levels - the number of octree levels. This is left up to the user and the service will not try to figure out the optimum value for the number of octree levels.&lt;/li>
&lt;li>voxelSize - specifies the voxel size in um.&lt;/li>
&lt;li>tiffOctreeContainerImage - tiff octree container image. Not that the format is slightly different for specifying the image name if docker is used or if singularity is used. Since singularity supports docker images, if singularity runtime is used you need to explictily specify that the image is a docker image.&lt;/li>
&lt;li>ktxOctreeContainerImage - ktx octree container image. See above regarding the format based on container processor type.&lt;/li>
&lt;li>subtreeLengthForSubjobSplitting - this parameter applies only for the ktx processor and it tells the service how to split the job and it has a default value of 5. The conversion process typically starts at a certain node and it performs tiff to ktx conversion for a specified number of levels. If you start a process at the root and convert all levels the job may take a while so if you want you have the option to parallelize it by going only for a limited number of levels from the root and start new jobs from all nodes at the level equal with the subtree depth. For example if you have 8 levels and you set &lt;code>subtreeLengthForSubjobSplitting&lt;/code> to &lt;code>3&lt;/code> then KTX conversion will start &lt;code>1 + 8^3 + 8^6 = 1 + 512 + 262144 = 262657&lt;/code> jobs with the following parameters:
&lt;code>&amp;quot;&amp;quot; 3, &amp;quot;111&amp;quot; 3, &amp;quot;112&amp;quot; 3, ..., &amp;quot;118&amp;quot; 3, ..., &amp;quot;888&amp;quot; 3, ..., &amp;quot;111111&amp;quot; 3, ..., &amp;quot;888888&amp;quot; 3&lt;/code>
If you leave the default (&lt;code>subtreeLengthForSubjobSplitting=5&lt;/code>) then the KTX conversion will start only &lt;code>1 + 8^5 = 32769&lt;/code> jobs (&lt;code>&amp;quot;11111&amp;quot; 5, ..., &amp;quot;88888&amp;quot; 5&lt;/code>)&lt;/li>
&lt;/ul>
&lt;p>Note that the service invocation requires authentication so before you invoke it, you need to obtain an JWS token from the authentication service - see the &lt;a href="../swarmdeployment">Verify Functionality section on this page&lt;/a>.&lt;/p></description></item><item><title>Docs: Troubleshooting</title><link>https://hortacloud.janelia.org/docs/administration/bare_metal/troubleshooting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://hortacloud.janelia.org/docs/administration/bare_metal/troubleshooting/</guid><description>
&lt;h2 id="docker">Docker&lt;/h2>
&lt;p>These are some useful commands for troubleshooting Docker:&lt;/p>
&lt;p>View logs for Docker daemon&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>sudo journalctl -fu docker
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Restart the Docker daemon&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>sudo systemctl restart docker
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Remove all Docker objects, including unused containers/networks/etc.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>sudo docker system prune -a
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="swarm-gui">Swarm GUI&lt;/h2>
&lt;p>If you would like to see the Swarm&amp;rsquo;s status in a web-based GUI, we recommend installing &lt;a href="https://swarmpit.io">Swarmpit&lt;/a>. It&amp;rsquo;s a single command to deploy, and it works well with the JACS stack.&lt;/p>
&lt;h2 id="common-issues">Common issues&lt;/h2>
&lt;h3 id="config-variable-not-set">config variable not set&lt;/h3>
&lt;p>If you see a lot of errors or warnings similar to the ones below, first check that the &lt;code>.env&lt;/code> file was generated correctly - it should have all environment variables from .env.config, present and set. If it is not just remove it and try the commands again. It is possible that you may have run a command like &lt;code>./manage.sh init-filesystems&lt;/code> before the swarm cluster was available.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>WARN&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>0000&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> The &lt;span style="color:#4e9a06">&amp;#34;CONFIG_DIR&amp;#34;&lt;/span> variable is not set. Defaulting to a blank string.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>WARN&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>0000&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> The &lt;span style="color:#4e9a06">&amp;#34;DATA_DIR&amp;#34;&lt;/span> variable is not set. Defaulting to a blank string.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>WARN&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>0000&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> The &lt;span style="color:#4e9a06">&amp;#34;DB_DIR&amp;#34;&lt;/span> variable is not set. Defaulting to a blank string.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>WARN&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>0000&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> The &lt;span style="color:#4e9a06">&amp;#34;BACKUPS_DIR&amp;#34;&lt;/span> variable is not set. Defaulting to a blank string.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>WARN&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>0000&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> The &lt;span style="color:#4e9a06">&amp;#34;CERT_SUBJ&amp;#34;&lt;/span> variable is not set. Defaulting to a blank string.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>WARN&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>0000&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> The &lt;span style="color:#4e9a06">&amp;#34;DEPLOYMENT&amp;#34;&lt;/span> variable is not set. Defaulting to a blank string.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>WARN&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>0000&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> The &lt;span style="color:#4e9a06">&amp;#34;MONGODB_SECRET_KEY&amp;#34;&lt;/span> variable is not set. Defaulting to a blank string.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>WARN&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>0000&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> The &lt;span style="color:#4e9a06">&amp;#34;API_GATEWAY_EXPOSED_HOST&amp;#34;&lt;/span> variable is not set. Defaulting to a blank string.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>WARN&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>0000&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> The &lt;span style="color:#4e9a06">&amp;#34;RABBITMQ_EXPOSED_HOST&amp;#34;&lt;/span> variable is not set. Defaulting to a blank string.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>WARN&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>0000&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> The &lt;span style="color:#4e9a06">&amp;#34;RABBITMQ_USER&amp;#34;&lt;/span> variable is not set. Defaulting to a blank string.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>WARN&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>0000&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> The &lt;span style="color:#4e9a06">&amp;#34;RABBITMQ_PASSWORD&amp;#34;&lt;/span> variable is not set. Defaulting to a blank string.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>WARN&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>0000&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> The &lt;span style="color:#4e9a06">&amp;#34;MAIL_SERVER&amp;#34;&lt;/span> variable is not set. Defaulting to a blank string.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>WARN&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>0000&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> The &lt;span style="color:#4e9a06">&amp;#34;NAMESPACE&amp;#34;&lt;/span> variable is not set. Defaulting to a blank string.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>WARN&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>0000&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> The &lt;span style="color:#4e9a06">&amp;#34;REDUNDANT_STORAGE&amp;#34;&lt;/span> variable is not set. Defaulting to a blank string.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>WARN&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>0000&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> The &lt;span style="color:#4e9a06">&amp;#34;REDUNDANT_STORAGE&amp;#34;&lt;/span> variable is not set. Defaulting to a blank string.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>WARN&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>0000&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> The &lt;span style="color:#4e9a06">&amp;#34;NON_REDUNDANT_STORAGE&amp;#34;&lt;/span> variable is not set. Defaulting to a blank string.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>WARN&lt;span style="color:#ce5c00;font-weight:bold">[&lt;/span>0000&lt;span style="color:#ce5c00;font-weight:bold">]&lt;/span> The &lt;span style="color:#4e9a06">&amp;#34;NON_REDUNDANT_STORAGE&amp;#34;&lt;/span> variable is not set. Defaulting to a blank string.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="network-not-found">&amp;ldquo;network not found&amp;rdquo;&lt;/h3>
&lt;p>If you see an intermittent error like this, just retry the command again:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>failed to create service jacs-cm_jacs-sync: Error response from daemon: network jacs-cm_jacs-net not found
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="bind-errors-during-init-filesystems">bind errors during init-filesystems&lt;/h3>
&lt;p>If during &lt;code>init-filesystems&lt;/code> you see an error that the config folder could not be bound on a particular node of the swarm cluster, make sure you did not forget to create the config and db directories on each node that is part of the swarm. The directories must exist in order for docker to be able to mount the corresponding volumes.
After you created all folders if you already ran &lt;code>./manage.sh init-filesystems&lt;/code> and it failed before you run it again stop it using&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>./manage.sh stop
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>and then you can try to re-run it&lt;/p>
&lt;h2 id="restful-services">RESTful services&lt;/h2>
&lt;p>You can access the RESTful services from the command line. Obtain a JWT token like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>./manage.sh login
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The default admin account is called &amp;ldquo;root&amp;rdquo; with password &amp;ldquo;root&amp;rdquo; for deployments with self-contained authentication.&lt;/p>
&lt;p>Now you can access any of the RESTful APIs on the gateway, for instance:&lt;/p>
&lt;pre tabindex="0">&lt;code>export TOKEN=&amp;lt;enter token here&amp;gt;
curl -k --request GET --url https://${API_GATEWAY_EXPOSED_HOST}/SCSW/JACS2AsyncServices/v2/services/metadata --header &amp;#34;Content-Type: application/json&amp;#34; --header &amp;#34;Authorization: Bearer $TOKEN&amp;#34;
&lt;/code>&lt;/pre></description></item></channel></rss>